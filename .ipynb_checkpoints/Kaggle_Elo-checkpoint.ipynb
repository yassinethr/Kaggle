{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6bf8eh3A7fj"
   },
   "source": [
    " <h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d4II046eKPf9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, GridSearchCV, ShuffleSplit, learning_curve\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sP6ibP8ak08t"
   },
   "outputs": [],
   "source": [
    "df_merchants = pd.read_csv(DATA_PATH + 'merchants.csv')\n",
    "df_hist = pd.read_csv(DATA_PATH + 'historical_transactions.csv')\n",
    "df_new = pd.read_csv(DATA_PATH + 'new_merchant_transactions.csv')\n",
    "df_train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "df_sample_sub = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HuIhMfmLhFf"
   },
   "source": [
    "<h2>Utils functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2eaMp11qbRMK"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    cv_sets = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "    scoring = 'neg_mean_squared_error'\n",
    "    ylim=(-100.0, 5)\n",
    "\n",
    "    grid_srch = GridSearchCV(model, param_grid=param_grid, cv=cv_sets, scoring=scoring)\n",
    "    grid_srch.fit(X_train, y_train)\n",
    "\n",
    "    # Best model\n",
    "    clf = grid_srch.best_estimator_      \n",
    "    print('Best model ==> ', clf)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('-- Test: Fitting done')\n",
    "\n",
    "    # Define prediction set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('-- Test: Prediction done')\n",
    "\n",
    "    print('TEST RMSE = ', mean_squared_error(y_test, y_pred)**0.5)        \n",
    "\n",
    "    return {'model': clf}\n",
    "\n",
    "\n",
    "def binarize(df):\n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0})\n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_merchants_df(df):\n",
    "\n",
    "    X_clust = df.drop('merchant_id', axis=1)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_clust = pd.get_dummies(X_clust, columns=['most_recent_sales_range', 'most_recent_purchases_range'])\n",
    "\n",
    "    for col in ['category_1', 'category_4']:\n",
    "        X_clust[col] = X_clust[col].apply(lambda x: ['N', 'Y'].index(x))\n",
    "\n",
    "    for col in ['avg_purchases_lag6', 'avg_purchases_lag12', 'avg_purchases_lag3']:\n",
    "        X_clust[col] = X_clust[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    for col in ['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12', 'avg_purchases_lag3', 'avg_purchases_lag6', 'avg_purchases_lag12']:\n",
    "        X_clust[col] = X_clust[col].fillna(X_clust[col].mean())\n",
    "\n",
    "    X_clust['category_2'] = X_clust['category_2'].fillna(X_clust['category_2'].mode()[0])\n",
    "\n",
    "    X_pca = pca.fit_transform(X_clust)\n",
    "\n",
    "    reduced_data = pd.DataFrame(X_pca, columns = ['dim_1', 'dim_2'])\n",
    "\n",
    "    outliers_list  = []\n",
    "\n",
    "    # For each feature find the data points with extreme high or low values\n",
    "    for feature in reduced_data.keys():\n",
    "\n",
    "        Q1 = np.percentile(reduced_data[feature], 25)\n",
    "        Q3 = np.percentile(reduced_data[feature], 75)\n",
    "\n",
    "        step = 1.5 * (Q3 - Q1)\n",
    "\n",
    "      # Display the outliers\n",
    "        outliers_df = reduced_data[~((reduced_data[feature] >= Q1 - step) & (reduced_data[feature] <= Q3 + step))]\n",
    "        print(\"{} Data points considered outliers for the feature '{}':\".format(len(outliers_df), feature))\n",
    "        outliers_list.append(list(outliers_df.index))\n",
    "\n",
    "    outliers_list = list(itertools.chain.from_iterable(outliers_list))\n",
    "\n",
    "    reduced_data_no_outliers = reduced_data.drop(reduced_data.index[outliers_list]).reset_index(drop = True)\n",
    "    merchant_ids_col = df['merchant_id'].drop(reduced_data.index[outliers_list])\n",
    "\n",
    "    df_merchants_final = pd.DataFrame()\n",
    "    df_merchants_final['merchant_id'] = merchant_ids_col\n",
    "    df_merchants_final['dim_1'] = reduced_data_no_outliers['dim_1']\n",
    "    df_merchants_final['dim_2'] = reduced_data_no_outliers['dim_2']\n",
    "\n",
    "    return df_merchants_final\n",
    "\n",
    "\n",
    "def aggregate_transactions(history):\n",
    "\n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "\n",
    "    agg_func = {        \n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean'],\n",
    "        'category_2_2.0': ['mean'],\n",
    "        'category_2_3.0': ['mean'],\n",
    "        'category_2_4.0': ['mean'],\n",
    "        'category_2_5.0': ['mean'],\n",
    "        'category_3_A': ['mean'],\n",
    "        'category_3_B': ['mean'],\n",
    "        'category_3_C': ['mean'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'state_id': ['nunique'],\n",
    "        'city_id': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std'],\n",
    "        'purchase_date': [np.ptp, 'min', 'max'],\n",
    "        'month_lag': ['mean', 'max', 'min', 'std'],\n",
    "        'dim_1': ['mean', 'max', 'min', 'std'],\n",
    "        'dim_2': ['mean', 'max', 'min', 'std']\n",
    "    }\n",
    "\n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "\n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='transactions_count'))\n",
    "\n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n",
    "\n",
    "    return agg_history\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "\n",
    "    df = pd.merge(df, df_hist, on='card_id', how='left')\n",
    "    df = pd.merge(df, df_new, on='card_id', how='left')\n",
    "\n",
    "    cols_to_drop = ['hist_authorized_flag_sum',\n",
    "                  'hist_merchant_category_id_nunique',\n",
    "                  'hist_month_lag_std',\n",
    "                  'hist_purchase_amount_max',\n",
    "                  'hist_purchase_amount_mean',\n",
    "                  'hist_purchase_amount_std',\n",
    "                  'hist_purchase_date_max',\n",
    "                  'hist_subsector_id_nunique',\n",
    "                  'new_authorized_flag_sum',\n",
    "                  'new_category_2_3.0_mean',\n",
    "                  'new_category_2_4.0_mean',\n",
    "                  'new_category_2_5.0_mean',\n",
    "                  'new_category_3_A_mean',\n",
    "                  'new_merchant_category_id_nunique',\n",
    "                  'new_merchant_id_nunique',\n",
    "                  'new_purchase_amount_std',\n",
    "                  'new_purchase_date_max',\n",
    "                  'new_purchase_date_min',\n",
    "                  'new_purchase_month_max',\n",
    "                  'new_purchase_month_min',\n",
    "                  'new_subsector_id_nunique']\n",
    "\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    for col in df.columns.tolist():\n",
    "        if col.startswith('new') or 'dim_2' in col or 'dim_1' in col:    \n",
    "            df[col] = df[col].fillna((df[col].mean()))\n",
    "\n",
    "    cols_to_log = ['hist_installments_max',\n",
    "                'hist_installments_std',\n",
    "                'hist_purchase_amount_min',\n",
    "                'hist_purchase_amount_sum',\n",
    "                'new_installments_max',\n",
    "                'new_installments_std',\n",
    "                'new_purchase_amount_mean',\n",
    "                'new_purchase_amount_min']\n",
    "\n",
    "    for col in cols_to_log:\n",
    "        df[col] = df[col].apply(lambda x: np.log(np.abs(x)+1))\n",
    "\n",
    "    df = df.drop(['hist_installments_std', 'new_authorized_flag_mean'], axis=1)\n",
    "\n",
    "    df['first_active_year'] = pd.to_datetime(df['first_active_month']).dt.year\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month']).dt.month\n",
    "\n",
    "    df['first_active_year'] = df['first_active_year'].fillna(df['first_active_year'].median())\n",
    "    df['first_active_month'] = df['first_active_month'].fillna(df['first_active_month'].median())\n",
    "\n",
    "    df = df.drop('card_id', axis=1)\n",
    "    df['hist_purchase_amount_sum'] = df['hist_purchase_amount_sum'].fillna((df['hist_purchase_amount_sum'].mean()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyfNTuqSbTcr"
   },
   "source": [
    "<h2>Feature Engineering</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CJdkMNEZQejH",
    "outputId": "2af29f21-d417-42a2-e759-8741f2df8b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Data points considered outliers for the feature 'dim_1':\n",
      "3087 Data points considered outliers for the feature 'dim_2':\n"
     ]
    }
   ],
   "source": [
    "df_merchants_reduced = reduce_merchants_df(df_merchants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "03u1_qibb_Oe"
   },
   "outputs": [],
   "source": [
    "df_hist = binarize(df_hist)\n",
    "df_new = binarize(df_new)\n",
    "\n",
    "df_hist = pd.get_dummies(df_hist, columns=['category_2', 'category_3'])\n",
    "df_new = pd.get_dummies(df_new, columns=['category_2', 'category_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkiyElEUVNAK"
   },
   "source": [
    "<h3>Retrieving them back (after mem restart)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ndJU4Pd7QUcb"
   },
   "outputs": [],
   "source": [
    "df_hist = pd.merge(df_hist, df_merchants_reduced, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QJbUenxXRB8o"
   },
   "outputs": [],
   "source": [
    "df_new = pd.merge(df_new, df_merchants_reduced, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q1kYS_MxbFJx"
   },
   "outputs": [],
   "source": [
    "df_hist['purchase_month'] = pd.to_datetime(df_hist['purchase_date']).dt.month\n",
    "df_new['purchase_month'] = pd.to_datetime(df_new['purchase_date']).dt.month\n",
    "\n",
    "df_hist = aggregate_transactions(df_hist)\n",
    "df_hist.columns = ['hist_' + c if c != 'card_id' else c for c in df_hist.columns]\n",
    "\n",
    "df_new = aggregate_transactions(df_new)\n",
    "df_new.columns = ['new_' + c if c != 'card_id' else c for c in df_new.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xxAeLJx0cpBj"
   },
   "outputs": [],
   "source": [
    "df_train = process_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bwjca1sVcgYi"
   },
   "outputs": [],
   "source": [
    "df_test = process_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USoPwZXQOS9t"
   },
   "source": [
    "<h3>Saving processed dataframes for GPU reuse</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sD4j92sPfTw0"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv(DATA_PATH + 'df_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8PZp-r-Ki9dM"
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(DATA_PATH + 'df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eTkOsywUc5T"
   },
   "source": [
    "<h2>Reading saved processed dataframes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqEqT3qJUb_-"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + 'df_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Jc1XQqZDGzv8"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(DATA_PATH + 'df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt2seUYKoH2Q"
   },
   "source": [
    "**Correlation Features Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-vMeTvtPU0W2"
   },
   "outputs": [],
   "source": [
    "# A CHECKER !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "correlations = df_train.drop('target', axis=1).apply(lambda x: x.corr(df_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "rpcZmWwvlTpj",
    "outputId": "b56459e1-4d83-411f-dc33-fec18a68f1ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2ad1e510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations.plot.bar(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReshiN8asuCx"
   },
   "source": [
    "**Remove least correlated features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1054
    },
    "colab_type": "code",
    "id": "HeeGrw75stZ2",
    "outputId": "349e3903-c651-46b3-ea1f-87253df50f00",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_dim_2_mean',\n",
       " 'new_dim_1_mean',\n",
       " 'new_category_2_1.0_mean',\n",
       " 'hist_category_2_2.0_mean',\n",
       " 'new_category_2_2.0_mean',\n",
       " 'Unnamed: 0',\n",
       " 'hist_month_lag_min',\n",
       " 'hist_category_2_4.0_mean',\n",
       " 'hist_dim_2_mean',\n",
       " 'hist_dim_1_mean',\n",
       " 'hist_category_2_1.0_mean',\n",
       " 'new_dim_1_std',\n",
       " 'new_dim_2_std',\n",
       " 'hist_dim_1_std',\n",
       " 'hist_dim_2_std',\n",
       " 'hist_month_lag_mean',\n",
       " 'hist_dim_1_min',\n",
       " 'hist_dim_2_min',\n",
       " 'hist_category_3_C_mean',\n",
       " 'new_category_3_B_mean',\n",
       " 'feature_2',\n",
       " 'hist_city_id_nunique',\n",
       " 'hist_purchase_date_ptp',\n",
       " 'hist_category_2_5.0_mean',\n",
       " 'feature_3',\n",
       " 'hist_dim_1_max',\n",
       " 'hist_dim_2_max',\n",
       " 'hist_merchant_id_nunique',\n",
       " 'new_purchase_amount_sum',\n",
       " 'new_installments_min',\n",
       " 'hist_state_id_nunique',\n",
       " 'hist_transactions_count',\n",
       " 'hist_purchase_amount_min',\n",
       " 'new_purchase_date_ptp',\n",
       " 'new_purchase_amount_min',\n",
       " 'new_purchase_amount_mean',\n",
       " 'hist_installments_mean',\n",
       " 'feature_1',\n",
       " 'hist_category_2_3.0_mean',\n",
       " 'hist_purchase_month_mean',\n",
       " 'hist_month_lag_max',\n",
       " 'new_dim_1_min',\n",
       " 'new_dim_2_min',\n",
       " 'new_dim_1_max',\n",
       " 'new_dim_2_max',\n",
       " 'hist_installments_max',\n",
       " 'new_month_lag_std',\n",
       " 'hist_category_3_A_mean',\n",
       " 'new_installments_std',\n",
       " 'hist_purchase_amount_sum',\n",
       " 'hist_category_3_B_mean',\n",
       " 'new_category_1_mean',\n",
       " 'new_month_lag_max',\n",
       " 'hist_installments_sum',\n",
       " 'new_installments_sum',\n",
       " 'first_active_month',\n",
       " 'new_city_id_nunique',\n",
       " 'new_purchase_month_std',\n",
       " 'new_installments_max',\n",
       " 'new_state_id_nunique',\n",
       " 'new_category_3_C_mean',\n",
       " 'hist_category_1_mean',\n",
       " 'new_installments_mean',\n",
       " 'hist_purchase_month_min',\n",
       " 'new_transactions_count',\n",
       " 'hist_installments_min',\n",
       " 'first_active_year',\n",
       " 'hist_authorized_flag_mean',\n",
       " 'new_category_1_sum',\n",
       " 'hist_category_1_sum',\n",
       " 'new_month_lag_min',\n",
       " 'new_month_lag_mean',\n",
       " 'hist_purchase_month_max',\n",
       " 'new_purchase_amount_max',\n",
       " 'hist_purchase_date_min',\n",
       " 'new_purchase_month_mean',\n",
       " 'hist_purchase_month_std']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(correlations).sort_values().index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7BYS1NCGITV"
   },
   "source": [
    "<h3>Try : Removing least correlated features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less_correlated_cols = ['new_dim_2_mean',\n",
    "                         'new_dim_1_mean',\n",
    "                         'new_category_2_1.0_mean',\n",
    "                         'hist_category_2_2.0_mean',\n",
    "                         'new_category_2_2.0_mean',\n",
    "                         'Unnamed: 0',\n",
    "                         'hist_month_lag_min',\n",
    "                         'hist_category_2_4.0_mean',\n",
    "                         'hist_dim_2_mean',\n",
    "                         'hist_dim_1_mean',\n",
    "                         'hist_category_2_1.0_mean',\n",
    "                         'new_dim_1_std',\n",
    "                         'new_dim_2_std',\n",
    "                         'hist_dim_1_std',\n",
    "                         'hist_dim_2_std',\n",
    "                         'hist_month_lag_mean',\n",
    "                         'hist_dim_1_min',\n",
    "                         'hist_dim_2_min',\n",
    "                         'hist_category_3_C_mean',\n",
    "                         'new_category_3_B_mean',\n",
    "                         'feature_2',\n",
    "                         'hist_city_id_nunique',\n",
    "                         'hist_purchase_date_ptp',\n",
    "                         'hist_category_2_5.0_mean',\n",
    "                         'feature_3',\n",
    "                         'hist_dim_1_max',\n",
    "                         'hist_dim_2_max',\n",
    "                         'hist_merchant_id_nunique',\n",
    "                         'new_purchase_amount_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['new_category_3_C_mean',\n",
    "                     'hist_category_1_mean',\n",
    "                     'new_installments_mean',\n",
    "                     'hist_purchase_month_min',\n",
    "                     'new_transactions_count',\n",
    "                     'hist_installments_min',\n",
    "                     'first_active_year',\n",
    "                     'hist_authorized_flag_mean',\n",
    "                     'new_category_1_sum',\n",
    "                     'hist_category_1_sum',\n",
    "                     'new_month_lag_min',\n",
    "                     'new_month_lag_mean',\n",
    "                     'hist_purchase_month_max',\n",
    "                     'new_purchase_amount_max',\n",
    "                     'hist_purchase_date_min',\n",
    "                     'new_purchase_month_mean',\n",
    "                     'hist_purchase_month_std', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLMiA61Dll3Y"
   },
   "outputs": [],
   "source": [
    "y = df_train.pop('target')\n",
    "X = df_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hg1_EV1bBD2Y"
   },
   "source": [
    "**ML models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best model ==> ', Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=42,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False))\n",
      "-- Test: Fitting done\n",
      "-- Test: Prediction done\n",
      "('TEST RMSE = ', 3.8430823794473468)\n"
     ]
    }
   ],
   "source": [
    "model = Lasso(random_state=42)\n",
    "param_grid = {'alpha': [0.1, 0.5, 1]}\n",
    "lasso = evaluate_model(model, param_grid, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best model ==> ', DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=10,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=42, splitter='best'))\n",
      "-- Test: Fitting done\n",
      "-- Test: Prediction done\n",
      "('TEST RMSE = ', 3.7907972148014784)\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeRegressor(random_state=42)\n",
    "param_grid = {'max_features': [3, 5, 10],\n",
    "              'max_depth': [5, 10, 20]}\n",
    "dt = evaluate_model(model, param_grid, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "560RAGd0ot1K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best model ==> ', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:linear', random_state=42,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1))\n",
      "-- Test: Fitting done\n",
      "-- Test: Prediction done\n",
      "('TEST RMSE = ', 3.7345605567590474)\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(random_state=42)\n",
    "param_grid = {'n_estimators': [50, 100],\n",
    "              'max_depth': [5, 10],\n",
    "              'learning_rate': [0.01, 0.1, 1]}\n",
    "gb = evaluate_model(model, param_grid, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFdN0HlLxxPx"
   },
   "source": [
    "**Saving best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HN39bC7wmBbg"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "c72cQ8t4USgP"
   },
   "outputs": [],
   "source": [
    "filename = DATA_PATH + 'gb_v2.sav'\n",
    "pickle.dump(gb['model'], open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kkWLbwOCc3be"
   },
   "source": [
    "**Making predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHLtG9a1UaSc"
   },
   "outputs": [],
   "source": [
    "filename = DATA_PATH + 'gb_v2.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[['new_category_3_C_mean',\n",
    "                     'hist_category_1_mean',\n",
    "                     'new_installments_mean',\n",
    "                     'hist_purchase_month_min',\n",
    "                     'new_transactions_count',\n",
    "                     'hist_installments_min',\n",
    "                     'first_active_year',\n",
    "                     'hist_authorized_flag_mean',\n",
    "                     'new_category_1_sum',\n",
    "                     'hist_category_1_sum',\n",
    "                     'new_month_lag_min',\n",
    "                     'new_month_lag_mean',\n",
    "                     'hist_purchase_month_max',\n",
    "                     'new_purchase_amount_max',\n",
    "                     'hist_purchase_date_min',\n",
    "                     'new_purchase_month_mean',\n",
    "                     'hist_purchase_month_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7hGG-XJYePMY"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "u2LT6ID0dRA9"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\": df_sample_sub[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(DATA_PATH +\"submit_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT9uEAc-pdpn"
   },
   "source": [
    "**Model analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ioY5WFB0jbjJ",
    "outputId": "ff5550e0-e231-454c-b705-16418e137740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=nan, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=42,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "SEbkCz0hpfp9",
    "outputId": "a6b9e5fc-44f3-4f82-a75f-8c079aea7be6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2ad1e510>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.Series(model.feature_importances_, index=df_train.columns.tolist())\n",
    "  .nlargest(30)\n",
    "  .plot(kind='barh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ok0Ukwwvp7ur"
   },
   "source": [
    "<h2>Partial dependency plot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Kaggle Elo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
